import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
from utils.llm_utils import LLMClient
from config import DEFAULT_PROVIDER, DEFAULT_MODEL

class InsightAgent:
    def __init__(self, provider: str = DEFAULT_PROVIDER, model: str = DEFAULT_MODEL):
        self.llm_client = LLMClient(provider, model)
        
    def generate_summary(self, issues_df: pd.DataFrame, original_df: pd.DataFrame = None) -> str:
        """Generate comprehensive summary of detected issues"""
        if issues_df.empty:
            return "✅ No revenue leakage issues detected in your data. Your business appears to be operating efficiently!"
            
        # Generate different types of insights
        executive_summary = self._generate_executive_summary(issues_df)
        detailed_analysis = self._generate_detailed_analysis(issues_df, original_df)
        recommendations = self._generate_recommendations(issues_df)
        risk_assessment = self._generate_risk_assessment(issues_df)
        
        # Combine all insights
        full_summary = f"""
# 🚨 Revenue Leakage Analysis Report

## 📊 Executive Summary
{executive_summary}

## 🔍 Detailed Analysis
{detailed_analysis}

## 🎯 Recommendations
{recommendations}

## ⚠️ Risk Assessment
{risk_assessment}

---
*Report generated by ZeroLeak.AI - Protecting your revenue, one transaction at a time.*
"""
        
        return full_summary
        
    def _generate_executive_summary(self, issues_df: pd.DataFrame) -> str:
        """Generate high-level executive summary"""
        total_issues = len(issues_df)
        total_loss = issues_df['potential_loss'].sum()
        
        # Get top issues by severity
        critical_issues = issues_df[issues_df['severity'] == 'critical']
        high_issues = issues_df[issues_df['severity'] == 'high']
        
        # Get most common issue types
        top_issue_types = issues_df['issue_type'].value_counts().head(3)
        
        summary = f"""
**Total Issues Detected:** {total_issues}
**Potential Revenue Loss:** ${total_loss:,.2f}
**Critical Issues:** {len(critical_issues)}
**High Priority Issues:** {len(high_issues)}

**Top Issue Types:**
"""
        
        for issue_type, count in top_issue_types.items():
            summary += f"- {issue_type.replace('_', ' ').title()}: {count} occurrences\n"
            
        return summary
        
    def _generate_detailed_analysis(self, issues_df: pd.DataFrame, original_df: pd.DataFrame = None) -> str:
        """Generate detailed analysis of issues"""
        analysis = ""
        
        # Analyze by severity
        severity_analysis = issues_df.groupby('severity').agg({
            'potential_loss': ['sum', 'count'],
            'issue_type': 'count'
        }).round(2)
        
        analysis += "**Issues by Severity:**\n"
        for severity in ['critical', 'high', 'medium', 'low']:
            if severity in severity_analysis.index:
                loss = severity_analysis.loc[severity, ('potential_loss', 'sum')]
                count = severity_analysis.loc[severity, ('potential_loss', 'count')]
                analysis += f"- {severity.title()}: {count} issues, ${loss:,.2f} potential loss\n"
                
        # Analyze by issue type
        analysis += "\n**Issues by Type:**\n"
        issue_type_analysis = issues_df.groupby('issue_type').agg({
            'potential_loss': 'sum',
            'severity': 'count'
        }).sort_values('potential_loss', ascending=False)
        
        for issue_type, row in issue_type_analysis.head(5).iterrows():
            analysis += f"- {issue_type.replace('_', ' ').title()}: {row['severity']} occurrences, ${row['potential_loss']:,.2f} potential loss\n"
            
        # Customer impact analysis
        if 'customer' in issues_df.columns:
            customer_impact = issues_df.groupby('customer').agg({
                'potential_loss': 'sum',
                'issue_type': 'count'
            }).sort_values('potential_loss', ascending=False)
            
            analysis += "\n**Top Affected Customers:**\n"
            for customer, row in customer_impact.head(3).iterrows():
                analysis += f"- {customer}: {row['issue_type']} issues, ${row['potential_loss']:,.2f} potential loss\n"
                
        return analysis
        
    def _generate_recommendations(self, issues_df: pd.DataFrame) -> str:
        """Generate actionable recommendations"""
        prompt = f"""
You are a revenue optimization expert. Based on the following revenue leakage issues, provide 5-7 specific, actionable recommendations for a startup to fix these problems and prevent future revenue loss.

Issues detected:
{issues_df.to_string()}

Provide recommendations in this format:
1. **Immediate Action Required** - [specific action]
2. **Process Improvement** - [specific process change]
3. **System Enhancement** - [specific system improvement]
4. **Monitoring Setup** - [specific monitoring recommendation]
5. **Team Training** - [specific training recommendation]

Focus on practical, implementable solutions that a startup can execute quickly.
"""
        
        recommendations = self.llm_client.call_llm(prompt, temperature=0.7, max_tokens=800)
        return recommendations
        
    def _generate_risk_assessment(self, issues_df: pd.DataFrame) -> str:
        """Generate risk assessment"""
        total_loss = issues_df['potential_loss'].sum()
        critical_count = len(issues_df[issues_df['severity'] == 'critical'])
        high_count = len(issues_df[issues_df['severity'] == 'high'])
        
        # Calculate risk score
        risk_score = (critical_count * 3 + high_count * 2 + len(issues_df[issues_df['severity'] == 'medium'])) / len(issues_df) if len(issues_df) > 0 else 0
        
        if risk_score >= 2.5:
            risk_level = "🔴 HIGH RISK"
            risk_description = "Immediate attention required. Significant revenue at risk."
        elif risk_score >= 1.5:
            risk_level = "🟡 MEDIUM RISK"
            risk_description = "Moderate risk level. Should be addressed within 1-2 weeks."
        else:
            risk_level = "🟢 LOW RISK"
            risk_description = "Low risk level. Monitor and address as part of regular operations."
            
        assessment = f"""
**Risk Level:** {risk_level}
**Risk Score:** {risk_score:.2f}/3.0
**Risk Description:** {risk_description}

**Key Risk Factors:**
- Critical Issues: {critical_count}
- High Priority Issues: {high_count}
- Total Potential Loss: ${total_loss:,.2f}

**Timeline for Resolution:**
- Critical issues: Address within 24-48 hours
- High priority issues: Address within 1 week
- Medium priority issues: Address within 2 weeks
- Low priority issues: Monitor and address as needed
"""
        
        return assessment
        
    def generate_customer_insights(self, issues_df: pd.DataFrame) -> str:
        """Generate customer-specific insights"""
        if 'customer' in issues_df.columns:
            customer_analysis = issues_df.groupby('customer').agg({
                'potential_loss': 'sum',
                'issue_type': 'count',
                'severity': lambda x: (x == 'critical').sum()
            }).sort_values('potential_loss', ascending=False)
            
            prompt = f"""
Based on this customer analysis, provide insights about which customers are most at risk and what specific actions should be taken for each:

Customer Analysis:
{customer_analysis.to_string()}

Provide insights in this format:
**High-Risk Customers:**
- [Customer name]: [Specific issue and recommended action]

**Medium-Risk Customers:**
- [Customer name]: [Specific issue and recommended action]

**Prevention Strategies:**
- [General strategy for preventing similar issues]
"""
            
            return self.llm_client.call_llm(prompt, temperature=0.7, max_tokens=600)
        else:
            return "Customer analysis not available - customer column not found in data."
            
    def generate_trend_analysis(self, issues_df: pd.DataFrame) -> str:
        """Generate trend analysis if date information is available"""
        if 'date' in issues_df.columns:
            # Convert date column to datetime if it's not already
            try:
                issues_df['date'] = pd.to_datetime(issues_df['date'])
                
                # Group by date and analyze trends
                daily_issues = issues_df.groupby(issues_df['date'].dt.date).agg({
                    'potential_loss': 'sum',
                    'issue_type': 'count'
                })
                
                prompt = f"""
Based on this daily trend analysis, provide insights about patterns in revenue leakage:

Daily Issues Trend:
{daily_issues.to_string()}

Provide insights about:
1. **Trend Patterns** - Are issues increasing, decreasing, or stable?
2. **Peak Days** - Which days have the most issues?
3. **Seasonal Patterns** - Any weekly/monthly patterns?
4. **Root Cause Analysis** - What might be causing these patterns?
"""
                
                return self.llm_client.call_llm(prompt, temperature=0.7, max_tokens=500)
            except:
                return "Date analysis not available - unable to parse date column."
        else:
            return "Trend analysis not available - date column not found in data."
            
    def generate_benchmark_analysis(self, issues_df: pd.DataFrame, original_df: pd.DataFrame = None) -> str:
        """Generate benchmark analysis against industry standards"""
        total_issues = len(issues_df)
        total_loss = issues_df['potential_loss'].sum()
        
        # Calculate issue rate
        issue_rate = (total_issues / len(original_df) * 100) if original_df is not None and len(original_df) > 0 else 0
        
        prompt = f"""
As a revenue optimization expert, provide benchmark analysis for this startup:

**Current Metrics:**
- Total Issues: {total_issues}
- Total Potential Loss: ${total_loss:,.2f}
- Issue Rate: {issue_rate:.2f}% of transactions

**Industry Benchmarks:**
- Typical revenue leakage: 2-5% of revenue
- Failed payment rate: 1-3% of transactions
- Support-related churn: 15-25% of support tickets

Provide analysis comparing this startup's performance to industry standards and recommendations for improvement.
"""
        
        return self.llm_client.call_llm(prompt, temperature=0.7, max_tokens=600)

# Backward compatibility
def generate_summary(flagged_df):
    """Legacy function for backward compatibility"""
    agent = InsightAgent()
    return agent.generate_summary(flagged_df)